{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library which integrates code with operating system\n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# used to generate random vector of data/noise as input for the generator\n",
    "import random \n",
    "\n",
    "# helps us write user-frindly code, especially when dealing with varaibles within functions\n",
    "import argparse \n",
    "\n",
    "#main PyTorch framework and libraries we need to use\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np #used for most of our mathematical oprations\n",
    "\n",
    "#packages for creating our plots and animations\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Getting data and defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = dataroot = r'C:/Users/Ray/OneDrive/Desktop/Capstone Project/GAN_images/Crests_Raw' #enter file path to target folder here\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 12 #128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "# size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3 (use 1 for black & white)\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 500\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loading and plotting our intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,# the folder where the data is located\n",
    "                           #reshaping the data in the form we desire\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),#resizing\n",
    "                               transforms.CenterCrop(image_size), #cropping and centering\n",
    "                               transforms.ToTensor(), #tranforming image data to tensors\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #normalising the data\n",
    "                           ]))\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, #shuffing images so the  genrator doesn't overfit on the sequence\n",
    "                                         num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on. Basically telling the model to run on the GPU if its available.\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some random training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialising our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function which will intialise our weights\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1.0:\n",
    "        #initalisation of weights based on Normal distro\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02) #mean 0, std.dev 0.02\n",
    "    elif classname.find('BatchNorm') != -1.0: \n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Defining the Generator class & creating the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu): #defininng the initial function\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential( #squencing the main as a sequential neural network\n",
    "            # input is Z, going into a convolution\n",
    "            # ConvTranspose2d applies a 2D transposed convolution operator over an input image composed of several input planes.\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "            # we can change the keral size if we wish(third parameter)\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), \n",
    "            \n",
    "            #Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) \n",
    "            #as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "            #notice the input has to match the second arguement of the above function (the output channel)\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            \n",
    "            ##using the reLU as the activation function\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #repeating the above steps three more times for three different sizes\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            \n",
    "            #this leaves us with our desired output size\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    #creating a function for the forward propagation\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After defining the Generator in the previous step, we have to now create the Generator object\n",
    "\n",
    "#here we are asking our gpu (or cpu) to create our Generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Defining the Discriminator class & creating the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Discriminator is essentially the revese of the Generator, so the architecture is very similar\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            #Conv2d applies a 2D convolution over an input signal composed of several input planes.\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            \n",
    "            # Leaky ReLU has beenfis over regual ReLU for the discriminator (you can still use regular ReLU)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Repeating the  steps, like the Generator, except this time with descneding sizes\n",
    "            # This is because are taking the ouput of the Generator as an input, and shrinking it down to a data value\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            \n",
    "            #using a sigmoid function to determine whether the prediction is for real or fake\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Creating a binary cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on binary cross-entropy:\n",
    "# https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize the progression of the generator\n",
    "# torch.randn returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)[0,1)\n",
    "# https://pytorch.org/docs/stable/generated/torch.rand.html\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both Generator and Discriminator\n",
    "# Notes on Adam Optimisers:\n",
    "# https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.\n",
    "# Notes on TORCH.OPTIM:\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "\n",
    "        # (1) Update Discriminator network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "        ###########################\n",
    "\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size, 1), real_label, device=device, dtype=torch.float).view(-1)\n",
    "\n",
    "        # Forward pass real batch through Discriminator\n",
    "        output = netD(real_cpu).view(-1)\n",
    "\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "\n",
    "        # Calculate gradients for Discriminator in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "        # Generate fake image batch with Generator\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        # Classify all fake batch with Discriminator\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "\n",
    "        # Calculate Discriminator's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "\n",
    "        # Update Discriminator\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "\n",
    "        # (2) Update Generator network: maximize log(D(G(z)))\n",
    "\n",
    "        ###########################\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "        # Since we just updated Discriminator, perform another forward pass of all-fake batch through Discriminator\n",
    "        output = netD(fake).view(-1)\n",
    "\n",
    "        # Calculate Generator's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "\n",
    "        # Calculate gradients for Generator\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "\n",
    "        # Update Generator\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the Generator is doing by saving Generator's output on fixed_noise\n",
    "        # This saves the output of each epoch of genrated images in a list (so we can animate progression)\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "           # for item in fake: \n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "               # img_list.append(vutils.make_grid(item,normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Plotting the loss of the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Plotting the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the generated images from the final epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Animating the progression of generated images during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This step may not generate an animation if your dataset is too large\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=500, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
